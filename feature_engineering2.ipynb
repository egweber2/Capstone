{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lv_df = pd.read_csv('/Users/ethanweber/Downloads/LV_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fix_categories(x):\n",
    "    x = str(x)\n",
    "    x = x.replace('[', '').replace(']', '').replace(',', '').split(\"u'\")\n",
    "    x = map(lambda y: y.replace(\"'\", \"\"), x)\n",
    "    x = map(lambda y: y.replace('u\"', \"\").replace('\"', \"\").strip(), x)\n",
    "    if filter(None, x) != None: \n",
    "        return filter(None, x)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "lv_df['categories'] = map(fix_categories, lv_df['categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_big_cat(z):\n",
    "    catlist = ['Active Life', 'Arts & Entertainment', 'Automotive', 'Beauty & Spas', 'Education', 'Event Planning & Services', 'Financial Services', 'Food', 'Health & Medical', 'Home Services', 'Hotels & Travel', 'Local Flavor', 'Local Services',  'Nightlife', 'Pets', 'Professional Services', 'Public Services & Government', 'Real Estate', 'Religious Organizations', 'Restaurants', 'Shopping']\n",
    "    return filter(lambda x: x in catlist, z)\n",
    "lv_df['big_cat'] = map(get_big_cat, lv_df['categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_little_cat(z):\n",
    "    catlist = ['Active Life', 'Arts & Entertainment', 'Automotive', 'Beauty & Spas', 'Education', 'Event Planning & Services', 'Financial Services', 'Food', 'Health & Medical', 'Home Services', 'Hotels & Travel', 'Local Flavor', 'Local Services',  'Nightlife', 'Pets', 'Professional Services', 'Public Services & Government', 'Real Estate', 'Religious Organizations', 'Restaurants', 'Shopping']\n",
    "    return filter(lambda x: (x in catlist) == False, z)\n",
    "lv_df['categories'] = map(get_little_cat, lv_df['categories'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_excluder(z):\n",
    "    rmlist = ['Automotive', 'Education', 'Financial Services', 'Mass Media', 'Home Services', 'Local Services', 'Pets', 'Professional Services', 'Public Services & Government', 'Real Estate', 'Religious Organizations']\n",
    "    for i in rmlist:\n",
    "        if i in z:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "lv_df['rm'] = map(get_excluder, lv_df['big_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lv_df = lv_df[lv_df['rm'] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lv_df[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del lv_df2['index']\n",
    "del lv_df2['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lv_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "def cleaning_text(sentence):\n",
    "    sentence = str(sentence)\n",
    "    sentence=sentence.lower()\n",
    "    sentence=re.sub('[^\\w\\s]',' ', sentence) #removes punctuations\n",
    "    sentence=re.sub('_',' ', sentence) #removes punctuations\n",
    "    sentence=re.sub('\\d+',' ', sentence) #removes digits\n",
    "    cleaned=' '.join([w for w in sentence.split() if not w in stop]) # removes english stopwords\n",
    "    cleaned=' '.join([w for w , pos in pos_tag(cleaned.split()) if (pos == 'NN' or pos=='JJ' or pos=='JJR' or pos=='JJS' )])\n",
    "    \n",
    "#selecting only nouns and adjectives\n",
    "    cleaned=' '.join([w for w in cleaned.split() if not len(w)<=2 ]) #removes single lettered words and digits\n",
    "    cleaned=cleaned.strip()\n",
    "    return cleaned\n",
    "\n",
    "#lv_df['text'] = lv_df['text'].apply(lambda x: cleaning_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "lmtizer=nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer() # Create our stemmer \n",
    "stem_lem = lambda x: lmtizer.lemmatize(stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_stem_lem(x):\n",
    "    try:\n",
    "        x = map(lambda y: stem_lem(y), cleaning_text(x).split())\n",
    "        return x\n",
    "    except:\n",
    "        print 'Fuck'\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "lv_df = pd.read_csv('lv_df_final.csv', sep='|', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lv_df['text2'] = map(split_stem_lem, lv_df['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fix_text2(x):\n",
    "    x = str(x)\n",
    "    x = x.replace('[', '').replace(']', '').replace(',', '').split(\" \")\n",
    "    x = map(lambda y: y.replace(\"'\", \"\"), x)\n",
    "    x = map(lambda y: y.replace('u\"', \"\").replace('\"', \"\").strip(), x)\n",
    "    if filter(None, x) != None: \n",
    "        return filter(None, x)\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethanweber/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (0,4,6,17,18,32,34,36,38,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print 'going...'\n",
    "lv_df = pd.read_csv('lv_df_final.csv', sep='|', encoding='utf-8')\n",
    "print 'loaded'\n",
    "lv_df['text2'] = map(fix_text2, lv_df['text2'])\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-75d58c3327e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlv_df2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlv_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mlv_df2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/ethanweber/anaconda/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__delitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0;31m# there was no match, this call should raise the appropriate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m             \u001b[0;31m# exception:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0;31m# delete from the caches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ethanweber/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   3600\u001b[0m         \u001b[0mDelete\u001b[0m \u001b[0mselected\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3601\u001b[0m         \"\"\"\n\u001b[0;32m-> 3602\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3604\u001b[0m         \u001b[0mis_deleted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ethanweber/anaconda/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2134\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2136\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4433)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4279)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13742)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13696)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "lv_df2 = lv_df\n",
    "del lv_df2['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del lv_df2['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flattened = [val for sublist in list(lv_df2['text2']) for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = list(lv_df2['text2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clipped_corpus = gensim.utils.ClippedCorpus(corpus, max_docs=None) \n",
    "id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')\n",
    "lsi = gensim.models.lsimodel.LsiModel(corpus=clipped_corpus, id2word=id2word, num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.utils.ClippedCorpus object at 0x2616bf310>\n"
     ]
    }
   ],
   "source": [
    "print clipped_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recover_cat_list(x):\n",
    "    x = str(x).replace('[', '').replace(']', '').split(',')\n",
    "    x = map(lambda y: y.replace(\"'\", \"\"), x)\n",
    "    x = map(lambda y: y.replace('u\"', \"\").replace('\"', \"\").strip(), x)\n",
    "    if filter(None, x) != None: \n",
    "        return filter(None, x)\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lv_df['categories'] = map(recover_cat_list, lv_df['categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lv_df['big_cat'] = map(recover_cat_list, lv_df['big_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import tarfile\n",
    "import itertools\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "def flatten(x):\n",
    "    return sum(list(x), [])\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in gensim.utils.simple_preprocess(text) if token not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_list = list(lv_df2['text2'])\n",
    "#word_list = sum(, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector = next(iter(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'person'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lv_df['categories'] = map(fix_categories, lv_df['categories'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#catlist = ['Active Life', 'Arts & Entertainment', 'Automotive', 'Beauty & Spas', 'Education', 'Event Planning & Services', 'Financial Services', 'Food', 'Home Services', 'Health & Medical', 'Hotels & Travel', 'Local Flavor', 'Local Services', 'Mass Media', 'Nightlife', 'Pets', 'Professional Services', 'Public Services & Government', 'Real Estate', 'Religious Organizations', 'Restaurants', 'Shopping']\n",
    "#rmlist = ['Automotive', 'Education', 'Financial Services',  'Health & Medical', 'Mass Media', 'Home Services', 'Local Services', 'Pets', 'Professional Services', 'Public Services & Government', 'Real Estate', 'Religious Organizations']\n",
    "#new_list = ['Active Life', 'Arts & Entertainment', 'Beauty & Spas', 'Event Planning & Services', 'Food', 'Hotels & Travel', 'Local Flavor', 'Nightlife', 'Restaurants', 'Shopping']\n",
    "#print len(sorted((rmlist) + (new_list)))\n",
    "#print len(sorted(catlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lv_df.to_pickle('lv_df.pkl')  # where to save it, usually as a .pkl\n",
    "\n",
    "#lv_df = pd.read_pickle('lv_df.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lv_df2 = lv_df\n",
    "del lv_df2['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "active_df = lv_df2[map(lambda x: 'Active Life' in x, lv_df2['big_cat'])]\n",
    "arts_df = lv_df2[map(lambda x: 'Arts & Entertainment' in x, lv_df2['big_cat'])]\n",
    "beauty_df = lv_df2[map(lambda x: 'Beauty & Spas' in x, lv_df2['big_cat'])]\n",
    "events_df = lv_df2[map(lambda x: 'Event Planning & Services' in x, lv_df2['big_cat'])]\n",
    "food_df = lv_df2[map(lambda x: 'Food' in x, lv_df2['big_cat'])]\n",
    "hotels_df = lv_df2[map(lambda x: 'Hotels & Travel' in x, lv_df2['big_cat'])]\n",
    "local_df = lv_df2[map(lambda x: 'Local Flavor' in x, lv_df2['big_cat'])]\n",
    "nightlife_df = lv_df2[map(lambda x: 'Nightlife' in x, lv_df2['big_cat'])]\n",
    "restaurant_df = lv_df2[map(lambda x: 'Restaurants' in x, lv_df2['big_cat'])]\n",
    "shops_df = lv_df2[map(lambda x: 'Shopping' in x, lv_df2['big_cat'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "active_df.to_csv('active_df.csv', sep='|', encoding='utf-8')\n",
    "arts_df.to_csv('arts_df.csv', sep='|', encoding='utf-8')\n",
    "beauty_df.to_csv('beauty_df.csv', sep='|', encoding='utf-8')\n",
    "events_df.to_csv('events_df.csv', sep='|', encoding='utf-8')\n",
    "food_df.to_csv('food_df.csv', sep='|', encoding='utf-8')\n",
    "hotels_df.to_csv('hotels_df.csv', sep='|', encoding='utf-8')\n",
    "local_df.to_csv('local_df.csv', sep='|', encoding='utf-8')\n",
    "nightlife_df.to_csv('nightlife_df.csv', sep='|', encoding='utf-8')\n",
    "restaurant_df.to_csv('restaurant_df.csv', sep='|', encoding='utf-8')\n",
    "shops_df.to_csv('shops_df.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "active_df.to_pickle('./pickles/active_df.pkl')\n",
    "arts_df.to_pickle('./pickles/arts_df.pkl')\n",
    "beauty_df.to_pickle('./pickles/beauty_df.pkl')\n",
    "events_df.to_pickle('./pickles/events_df.pkl')\n",
    "food_df.to_pickle('./pickles/food_df.pkl')\n",
    "hotels_df.to_pickle('./pickles/hotels_df.pkl')\n",
    "local_df.to_pickle('./pickles/local_df.pkl')\n",
    "nightlife_df.to_pickle('./pickles/nightlife_df.pkl')\n",
    "restaurant_df.to_pickle('./pickles/restaurant_df.pkl')\n",
    "shops_df.to_pickle('./pickles/shops_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
    "                                stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iter_wiki(dump_file):\n",
    "    \"\"\"Yield each article from the Wikipedia dump, as a `(title, tokens)` 2-tuple.\"\"\"\n",
    "    ignore_namespaces = 'Wikipedia Category File Portal Template MediaWiki User Help Book Draft'.split()\n",
    "    for title, text, pageid in _extract_pages(smart_open(dump_file)):\n",
    "        text = filter_wiki(text)\n",
    "        tokens = tokenize(text)\n",
    "        if len(tokens) < 50 or any(title.startswith(ns + ':') for ns in ignore_namespaces):\n",
    "            continue  # ignore short articles and various meta-articles\n",
    "        yield title, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tf = tf_vectorizer.fit_transform(text_train)\n",
    "TF = TfidfTransformer()\n",
    "tf_idf = TF.fit_transform(tf)\n",
    "tf_idf = tf_idf.toarray() #convert sparse matrix to array\n",
    "#tf_idf.toarray()[:100, :100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LV_samplei.categories = map(lambda x: fix_categories(str(x)) , LV_samplei.categories)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#new_df['Biz_ID'] = pd.Series(list(set(LV_samplei.business_id)))\n",
    "#new_df['cat'] = pd.Series()\n",
    "#for i in range(len(new_df)):\n",
    "#    new_df['cat'][i] = LV_sample[LV_samplei.business_id ==  new_df['Biz_ID'][i]].categories.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "df_list = [active_df, arts_df, beauty_df, events_df, food_df, hotels_df, local_df, nightlife_df, restaurant_df, shops_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "active_dict = corpora.Dictionary(active_df.categories)\n",
    "arts_dict = corpora.Dictionary(arts_df.categories)\n",
    "beauty_dict = corpora.Dictionary(beauty_df.categories)\n",
    "events_dict = corpora.Dictionary(events_df.categories)\n",
    "food_dict = corpora.Dictionary(food_df.categories)\n",
    "hotels_dict = corpora.Dictionary(hotels_df.categories)\n",
    "local_dict = corpora.Dictionary(local_df.categories)\n",
    "nightlife_dict = corpora.Dictionary(nightlife_df.categories)\n",
    "restaurant_dict = corpora.Dictionary(restaurant_df.categories)\n",
    "shops_dict = corpora.Dictionary(shops_df.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_term_active = [active_dict.doc2bow(x) for x in active_df.categories]\n",
    "doc_term_arts = [arts_dict.doc2bow(x) for x in arts_df.categories]\n",
    "doc_term_beauty = [beauty_dict.doc2bow(x) for x in beauty_df.categories]\n",
    "doc_term_events = [events_dict.doc2bow(x) for x in events_df.categories]\n",
    "doc_term_food = [food_dict.doc2bow(x) for x in food_df.categories]\n",
    "doc_term_hotels = [hotels_dict.doc2bow(x) for x in hotels_df.categories]\n",
    "doc_term_local = [local_dict.doc2bow(x) for x in local_df.categories]\n",
    "doc_term_nightlife = [nightlife_dict.doc2bow(x) for x in nightlife_df.categories]\n",
    "doc_term_restaurant = [restaurant_dict.doc2bow(x) for x in restaurant_df.categories]\n",
    "doc_term_shops = [shops_dict.doc2bow(x) for x in shops_df.categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_active = Lda(doc_term_active, num_topics=3, id2word = active_dict, passes=3)\n",
    "print 'active done'\n",
    "lda_arts = Lda(doc_term_arts, num_topics=5, id2word = arts_dict, passes=3)\n",
    "print 'arts done'\n",
    "lda_beauty = Lda(doc_term_beauty, num_topics=3, id2word = beauty_dict, passes=3)\n",
    "print 'beauty done'\n",
    "lda_events = Lda(doc_term_events, num_topics=7, id2word = events_dict, passes=3)\n",
    "print 'events done'\n",
    "lda_food = Lda(doc_term_food, num_topics=10, id2word = food_dict, passes=3)\n",
    "print 'food done'\n",
    "lda_hotels = Lda(doc_term_hotels, num_topics=6, id2word = hotels_dict, passes=3)\n",
    "print 'hotels done'\n",
    "lda_local = Lda(doc_term_local, num_topics=3, id2word = local_dict, passes=3)\n",
    "print 'local done'\n",
    "lda_nightlife = Lda(doc_term_nightlife, num_topics=3, id2word = nightlife_dict, passes=3)\n",
    "print 'nightlife done'\n",
    "lda_restaurant = Lda(doc_term_restaurant, num_topics=45, id2word = restaurant_dict, passes=3)\n",
    "print 'restaurants done'\n",
    "lda_shops = Lda(doc_term_shops, num_topics=20, id2word = shops_dict, passes=3)\n",
    "print 'shops done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_active.save('./lda_pickles/lda_active.pkl')\n",
    "lda_arts.save('./lda_pickles/lda_arts.pkl')\n",
    "lda_beauty.save('./lda_pickles/lda_beauty.pkl')\n",
    "lda_events.save('./lda_pickles/lda_events.pkl')\n",
    "lda_food.save('./lda_pickles/lda_food.pkl')\n",
    "lda_hotels.save('./lda_pickles/lda_hotels.pkl')\n",
    "lda_local.save('./lda_pickles/lda_local.pkl')\n",
    "lda_nightlife.save('./lda_pickles/lda_nightlife.pkl')\n",
    "lda_restaurant.save('./lda_pickles/lda_restaurant.pkl')\n",
    "lda_shops.save('./lda_pickles/lda_shops.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_list = [lda_active, lda_arts, lda_beauty, lda_events, lda_food, lda_hotels, lda_local, lda_nightlife, lda_restaurant, lda_shops] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in lda_list:\n",
    "    print i\n",
    "    print i.print_topics(num_topics=3, num_words=3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As I want to avoid dealing with a huge sparse matrix, I will only select the top most reviewed books for our example \n",
    "\n",
    "most_reviewed_biz = pd.DataFrame({'count' : LV_test.groupby([\"name_x\"]).size()})\\\n",
    "                                    .reset_index().sort(['count'],ascending = False)\n",
    "\n",
    "most_reviewed_biz.head(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a function that collect the reviews of our common reviewers\n",
    "def get_biz_reviews(biz, common_reviewers):\n",
    "    mask = (LV_test.user_id.isin(common_reviewers)) & (LV_test.name_x==biz)\n",
    "    reviews = LV_test[mask].sort('user_id')\n",
    "    reviews = reviews[reviews.user_id.duplicated()==False]\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the scipy library to measure the pearson correlation coefficient\n",
    "from scipy.stats.stats import pearsonr\n",
    "def calculate_correlation(biz1, biz2):\n",
    "    # We start by finding the common reviewers\n",
    "    biz_1_reviewers = LV_test[LV_test.name_x == biz1].user_id\n",
    "    biz_2_reviewers = LV_test[LV_test.name_x == biz2].user_id\n",
    "    common_reviewers = set(biz_1_reviewers).intersection(biz_2_reviewers)\n",
    "\n",
    "    # Then we look for the reviews given by common reviewers\n",
    "    biz_1_reviews = get_biz_reviews(biz1, common_reviewers)\n",
    "    biz_2_reviews = get_biz_reviews(biz2, common_reviewers)\n",
    "    \n",
    "    # Calculate the Pearson Correlation Score\n",
    "    return pearsonr(biz_1_reviews.stars_x, biz_2_reviews.stars_x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting the list of the most reviewed biz\n",
    "\n",
    "top_biz = []\n",
    "\n",
    "for i in most_reviewed_biz.name_x[0:13]:\n",
    "    top_biz.append(i)\n",
    "\n",
    "    \n",
    "# calculate the correlation for our top biz\n",
    "correlation_coefficient = []\n",
    "\n",
    "for biz1 in top_biz:\n",
    "    print \"Calculating the correlations for:\", biz1\n",
    "    for biz2 in top_biz:\n",
    "        if biz1 != biz2:\n",
    "            row = [biz1, biz2] + [calculate_correlation(biz1, biz2)]\n",
    "            correlation_coefficient.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's look at what the table of correlation looks like\n",
    "cols = [\"biz_1\", \"biz_2\", \"Correlation\"]\n",
    "correlation_coefficient = pd.DataFrame(correlation_coefficient, columns=cols).sort('Correlation')\n",
    "correlation_coefficient.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_correlation(corr, biz1, biz2):\n",
    "    mask = (corr.biz_1==biz1) & (corr.biz_2==biz2)\n",
    "    row = corr[mask]\n",
    "    corr = row\n",
    "    return corr.sum(axis=1).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calc_correlation(correlation_coefficient,\"McDonald's\", \"Smashburger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
